{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b7551b-0a47-490c-a0dc-adc60d73f605",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT) Fine-Tuning for Image Classification\n",
    "\n",
    "The Vision Transformer (ViT) model was proposed in An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. Itâ€™s the first paper that successfully trains a Transformer encoder on ImageNet, attaining very good results compared to familiar convolutional architectures.\n",
    "\n",
    "- Refer for more details: https://huggingface.co/docs/transformers/en/model_doc/vit\n",
    "- Original Paper: https://arxiv.org/abs/2010.11929"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a15d5",
   "metadata": {},
   "source": [
    "About this Notebook\n",
    "```\n",
    "Developer: Chintan Patel\n",
    "Date: January 2025\n",
    "Description: This notebook demonstrates the fine-tuning of a Vision Transformer (ViT) model for image classification using the Hugging Face Transformers library. The model is trained on a custom dataset of images, and the training process includes evaluation and saving the best model.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9cb5a41-5fcc-47d0-b916-941821420906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor, Trainer, TrainingArguments\n",
    "\n",
    "import warnings\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5884214-8e8a-4b8f-95d5-e758cea0dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for training\n",
    "learning_rate = 0.0002\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 16\n",
    "seed = 42\n",
    "num_epochs = 20\n",
    "num_classes = 9\n",
    "root_folder =  r\"D:\\Chintan\\AI_capstone\\Data\\Images_300\"\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Enable mixed precision if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "931ede47-6c4c-4dc0-866a-e8e31cd3da25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the ViT feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "# Custom Dataset class for handling image data\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading and preprocessing images.\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of paths to the images.\n",
    "        labels (list): List of labels corresponding to the images.\n",
    "        feature_extractor (ViTFeatureExtractor): Feature extractor for preprocessing images.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, labels, feature_extractor):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Use the feature extractor to preprocess the image\n",
    "        encoding = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        # Return the processed data as a dictionary\n",
    "        return {\n",
    "            'pixel_values': encoding['pixel_values'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Get image paths and labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "# Loop over all disease folders in the root folder\n",
    "disease_folders = os.listdir(root_folder)\n",
    "class_map = {disease: idx for idx, disease in enumerate(disease_folders)}\n",
    "\n",
    "for disease in disease_folders:\n",
    "    disease_folder_path = os.path.join(root_folder, disease)\n",
    "    if os.path.isdir(disease_folder_path):\n",
    "        for img_name in os.listdir(disease_folder_path):\n",
    "            img_path = os.path.join(disease_folder_path, img_name)\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(class_map[disease])\n",
    "\n",
    "# Split into train and validation sets (80/20 split)\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(image_paths, labels, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ba25fb0-fb2c-4e38-b599-dcab807c61f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Type: <class '__main__.CustomImageDataset'>\n",
      "Eval Dataset Type: <class '__main__.CustomImageDataset'>\n",
      "Model Type: <class 'transformers.models.vit.modeling_vit.ViTForImageClassification'>\n"
     ]
    }
   ],
   "source": [
    "# Create Datasets\n",
    "train_dataset = CustomImageDataset(train_paths, train_labels, feature_extractor)\n",
    "val_dataset = CustomImageDataset(val_paths, val_labels, feature_extractor)\n",
    "\n",
    "# Load pre-trained ViT model\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# Check the types of datasets and model\n",
    "print(f\"Train Dataset Type: {type(train_dataset)}\")\n",
    "print(f\"Eval Dataset Type: {type(val_dataset)}\")\n",
    "print(f\"Model Type: {type(model)}\")\n",
    "\n",
    "# Update model's config with custom label mappings (id2label, label2id)\n",
    "model.config.id2label = {v: k for k, v in class_map.items()}\n",
    "model.config.label2id = class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "587582fc-f5c2-4c42-9c3e-2980c76f4ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=num_epochs,     # number of training epochs\n",
    "    per_device_train_batch_size=train_batch_size,  # batch size for training\n",
    "    per_device_eval_batch_size=eval_batch_size,    # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,                # log every 10 steps\n",
    "    evaluation_strategy=\"epoch\",     # evaluate every epoch\n",
    "    save_strategy=\"epoch\",           # save checkpoint every epoch\n",
    "    load_best_model_at_end=True,     # load the best model when finished training\n",
    "    metric_for_best_model=\"accuracy\",  # use accuracy to determine the best model\n",
    "    greater_is_better=True,\n",
    "    seed=seed,\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    learning_rate=learning_rate,\n",
    ")\n",
    "\n",
    "# Define the metric computation function\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Compute accuracy for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        p (EvalPrediction): Contains predictions and labels.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with accuracy score.\n",
    "    \"\"\"\n",
    "    preds = p.predictions.argmax(axis=1)\n",
    "    labels = p.label_ids\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecde3c2d-e2c1-42e3-b136-1a4df78b0de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments Type: <class 'transformers.training_args.TrainingArguments'>\n"
     ]
    }
   ],
   "source": [
    "# Check the types of the arguments being passed to the Trainer\n",
    "print(f\"Training Arguments Type: {type(training_args)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8e363aa-62f2-4995-bbe3-2d191c6e52b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\accelerate\\accelerator.py:482: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1360' max='1360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1360/1360 15:25, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.027100</td>\n",
       "      <td>1.912062</td>\n",
       "      <td>0.542593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.269800</td>\n",
       "      <td>1.212890</td>\n",
       "      <td>0.679630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.903600</td>\n",
       "      <td>0.967930</td>\n",
       "      <td>0.690741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.616000</td>\n",
       "      <td>0.873984</td>\n",
       "      <td>0.705556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.486100</td>\n",
       "      <td>0.946642</td>\n",
       "      <td>0.688889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.908524</td>\n",
       "      <td>0.705556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.421800</td>\n",
       "      <td>1.042436</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.331200</td>\n",
       "      <td>0.988834</td>\n",
       "      <td>0.692593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1.116554</td>\n",
       "      <td>0.690741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.190200</td>\n",
       "      <td>1.184392</td>\n",
       "      <td>0.696296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.105400</td>\n",
       "      <td>1.120795</td>\n",
       "      <td>0.707407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.110600</td>\n",
       "      <td>1.248664</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>1.240303</td>\n",
       "      <td>0.714815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>1.257495</td>\n",
       "      <td>0.720370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>1.322666</td>\n",
       "      <td>0.724074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>1.288821</td>\n",
       "      <td>0.735185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>1.314891</td>\n",
       "      <td>0.744444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.336431</td>\n",
       "      <td>0.740741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>1.346333</td>\n",
       "      <td>0.742593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>1.350332</td>\n",
       "      <td>0.742593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Checkpoint: ./results\\checkpoint-1156\n"
     ]
    }
   ],
   "source": [
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the model to train\n",
    "    args=training_args,                  # training arguments\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # function to compute metrics\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model\n",
    "# Save the best model manually after training, if needed:\n",
    "trainer.save_model('vit_finetuned_best_model')\n",
    "\n",
    "# To get the name of the best checkpoint:\n",
    "best_checkpoint = trainer.state.best_model_checkpoint\n",
    "print(f\"Best Model Checkpoint: {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb14facf-6814-4074-af17-cff09d30a279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\Chintan\\\\AI_capstone\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save it to your local directory\n",
    "feature_extractor.save_pretrained(r'D:\\Chintan\\AI_capstone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27cd36a1-8868-4888-a3e8-491a9cc38899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chintan\\miniconda3\\envs\\fooocus-api\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       20.0\n",
      "  eval_accuracy           =     0.7444\n",
      "  eval_loss               =     1.3149\n",
      "  eval_runtime            = 0:00:07.78\n",
      "  eval_samples_per_second =     69.366\n",
      "  eval_steps_per_second   =      4.368\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d41b721-5fce-4a42-a310-edee979a42d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and feature extractor\n",
    "model_path = './vit_finetuned_best_model'  # Path to the fine-tuned model\n",
    "model = ViTForImageClassification.from_pretrained(model_path)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "# Ensure that model is in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1232a5c3-dcd6-4f7b-b5b3-fdb235b7cf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "1. psoriasis: 98.87%\n",
      "2. f_infection: 0.51%\n",
      "3. eczema: 0.15%\n",
      "4. alopecia: 0.09%\n",
      "5. skincancer: 0.09%\n"
     ]
    }
   ],
   "source": [
    "# Load your image\n",
    "image_path = \"psoriasis1.jpg\"  # Test image of psoriasis\n",
    "image = Image.open(image_path).convert(\"RGB\")  # Ensure RGB format\n",
    "\n",
    "# Preprocess the image using the feature extractor\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get probabilities and predictions\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "top5_probs, top5_indices = torch.topk(probabilities, 5)\n",
    "\n",
    "# Get the class label mapping from the model config\n",
    "id2label = model.config.id2label  # This contains the mapping from index to class label\n",
    "\n",
    "# Print top 5 predictions with labels\n",
    "print(\"Top 5 predictions:\")\n",
    "for i, (prob, idx) in enumerate(zip(top5_probs, top5_indices)):\n",
    "    label = id2label[idx.item()]  # Directly access the label using the integer index\n",
    "    print(f\"{i+1}. {label}: {prob.item()*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
